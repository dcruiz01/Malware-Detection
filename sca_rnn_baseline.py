# dependencies
import numpy as np
import pandas as pd
import os
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential, load_model, Model
from tensorflow.keras.layers import SimpleRNN, LSTM, GRU, Bidirectional, Dense, InputLayer
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, Callback
from tensorflow.keras.optimizers import RMSprop

# global variables
SEQUENCE_LENGTH = 960 # 500ms capture rate
train_datapath = 'data_blend/train'
test_datapath = 'data_blend/test'

def prepare_data(datapath, seqlen):

    # read in data
    frames = []
    numfiles = 0
    for data_file in os.scandir(datapath):
        if (data_file.path.endswith('.csv') and data_file.is_file()):
            numfiles += 1
            df = pd.read_csv(data_file)
            df_cutoff = df[:df.shape[0] - df.shape[0] % seqlen].copy() # remove rows to ensure no sequences span two files
            print(f'Removed {df.shape[0] - df_cutoff.shape[0]} rows from {data_file.path}, Original Size: {df.shape[0]}')
            frames.append(df_cutoff)
    print(f'Number of files processed: {numfiles}')

    # concatenate all data into a single dataframe
    df_big = pd.concat(frames)

    # remove unneeded features 
    dropped_columns = ['Date', 'Time', 'OS', 'machine', 'activity'] # 'Label' column removed after label vector creation
    df_big.drop(columns = dropped_columns, inplace = True)

    # convert Yes/No columns to 1/0
    df_big.replace(['Yes', 'No'], ['1.0', '0.0'], inplace = True)

    # convert all values to float
    df_big = df_big.astype(float)

    # # shuffle sequences (NOT NECESSARY, data shuffled in .fit after reshape)
    # # source: (https://stackoverflow.com/questions/44159432/how-to-shuffle-groups-of-rows-of-a-pandas-dataframe)
    # np.random.shuffle(df_big.values.reshape(-1, SEQUENCE_LENGTH, df_big.shape[1]))

    # generate labels
    labels = []
    for i in range(0, len(df_big), seqlen):
        block = df_big['Label'][i:i+seqlen]
        if 1.0 in block:
            labels.append(1)
        else:
            labels.append(0)

    # convert labels to numpy array
    labels = np.array(labels)

    # remove 'Label' column
    df_big.drop(columns = 'Label', inplace = True)

    # convert data to numpy array
    data = np.array(df_big.to_numpy(dtype = 'float32'))
    data = data.reshape(-1, seqlen, 116) # reshape based on sequence length

    return data, labels

# generate training & test sets
xtrain, ytrain = prepare_data(train_datapath, SEQUENCE_LENGTH)
xtest, ytest = prepare_data(test_datapath, SEQUENCE_LENGTH)
print(xtrain.shape, ytrain.shape, xtest.shape, ytest.shape)


def create_model(seqlen, type):
    M = Sequential()
    if type == 1:
        model.add(SimpleRNN(16, input_shape = (seqlen,116), return_sequences = True))
        model.add(SimpleRNN(32, return_sequences = True))
        model.add(SimpleRNN(32, return_sequences = True))
        model.add(SimpleRNN(16))
    elif type == 2:
        model.add(LSTM(16, input_shape = (seqlen,116), return_sequences = True))
        model.add(LSTM(32, return_sequences = True))
        model.add(LSTM(32, return_sequences = True))
        model.add(LSTM(16))
    elif type == 3:
        model.add(LSTM(16, input_shape = (seqlen,116), return_sequences = True))
        model.add(LSTM(32, return_sequences = True))
        model.add(LSTM(32, return_sequences = True))
        model.add(LSTM(16))
    elif type == 4:
        model.add(GRU(16, input_shape = (seqlen,116), return_sequences = True))
        model.add(GRU(32, return_sequences = True))
        model.add(GRU(32, return_sequences = True))
        model.add(GRU(16))
    elif type == 5:
        model.add(GRU(16, input_shape = (seqlen,116), return_sequences = True))
        model.add(GRU(32, return_sequences = True))
        model.add(GRU(32, return_sequences = True))
        model.add(GRU(16))
    model.add(Dense(1, activation = 'sigmoid'))
    model.compile(loss = 'binary_crossentropy', optimizer = RMSprop(lr = .001), metrics = ['accuracy'])
    return M

# create model
model = create_model(SEQUENCE_LENGTH, model_type)

# callback list
A = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 10, min_delta = .0001, restore_best_weights=True)
B = ModelCheckpoint(filepath = 'models/RNN/RNN_model.hdf5', save_best_only = True, verbose = 1, monitor = 'val_loss', mode = 'min', )
C = ReduceLROnPlateau(monitor = 'val_loss', factor = .1, patience = 10, verbose = 1, min_delta = .001)

cb_list = [A, B, C]

# fit model
history = model.fit(xtrain, ytrain, validation_split = .10, shuffle = True, batch_size = 16, epochs = 100, verbose = 1, callbacks = cb_list)

# Plot training/validation accuracy
acc = history.history['accuracy']
vacc = history.history['val_accuracy']
epochs = range(1, len(acc) + 1)

plt.figure()
plt.plot(epochs, acc, 'b', label = 'Training Acc.')
plt.plot(epochs, vacc, 'g', label = 'Validation Acc.')
plt.title ('Training vs. Validation Accuracy')
plt.legend()
plt.show()

print('Maximum Validation Accuracy: ' + str(max(history.history['val_accuracy'])))

# Plot training/validation loss
loss = history.history['loss']
vloss = history.history['val_loss']
epochs = range(1, len(loss) + 1)

plt.figure()
plt.plot(epochs, loss, 'b', label = 'Training Loss')
plt.plot(epochs, vloss, 'g', label = 'Validation Loss')
plt.title ('Training vs. Validation Loss')
plt.legend()
plt.show()

print('Minimum Validation Loss: ' + str(min(history.history['val_loss'])))

# Evaluate on test data
trained_model = load_model('models/RNN/RNN_model.hdf5')
results = trained_model.evaluate(xtest, ytest, batch_size = 16, verbose = 1)
print("Results:\ntest loss, test acc:", results)